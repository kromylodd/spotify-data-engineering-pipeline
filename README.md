<<<<<<< HEAD
ðŸŽ§ Spotify Data Engineering Pipeline

A production-style end-to-end data engineering pipeline that ingests, transforms, and analyzes Spotify track data to produce analytics-ready datasets in a cloud data warehouse.

This project demonstrates a real-world data platform architecture including data ingestion, cleaning, transformation, optimization, orchestration, and visualization.

ðŸš€ Project Goal
=======
# ðŸŽ§ Spotify Data Engineering Pipeline

A production-grade, end-to-end Spotify data engineering pipeline that ingests raw track metadata, performs deterministic cleaning, stores data in a cloud data lake and warehouse, applies BigQuery transformations and optimizations, and exposes analytics via a Looker Studio dashboard.

This repository demonstrates cloud-native batch ingestion, workflow orchestration with Kestra, BigQuery optimization (partitioning & clustering), and a reproducible workflow suitable for portfolio presentation.
>>>>>>> 8164749 (docs: Add production-ready README)

The goal of this project is to simulate a modern data engineering workflow by building an automated pipeline that:

<<<<<<< HEAD
ingests raw music data

cleans and standardizes schemas

stores data in a cloud data warehouse
=======
**Project Goal**

- Build a reproducible, cloud-based batch pipeline that ingests raw Spotify CSV data, stores cleaned versions in Google Cloud Storage (GCS), loads and optimizes tables in BigQuery, and presents analytics in Looker Studio.
- Demonstrate orchestration with Kestra and deliver industry-standard data warehouse optimizations for cost-efficient analytics.
>>>>>>> 8164749 (docs: Add production-ready README)

optimizes storage for analytics queries

<<<<<<< HEAD
serves insights through dashboards

The pipeline transforms raw Spotify track metadata into structured datasets that enable efficient analysis of music trends and audio characteristics.

â— Problem Description
Problem

Music platforms generate large volumes of track metadata that must be efficiently stored, processed, and analyzed. However, raw datasets often contain inconsistent schemas, redundant columns, and are not optimized for analytical queries.

Without proper ingestion pipelines and warehouse optimization, querying music features such as popularity, genre distribution, and audio characteristics becomes slow and inefficient.
=======
**Problem Description**

- Analytics stacks often fail due to inconsistent ingestion (junk/unnamed columns), manual upload steps, and unoptimized warehouse schemas that produce slow and expensive queries.
- This project automates the full batch pipeline to eliminate manual steps and ensures repeatable, low-cost analytics by:
  - Using Kestra to orchestrate a multi-step DAG
  - Cleaning data deterministically with Pandas to remove junk columns
  - Keeping a versioned clean file in GCS
  - Loading and optimizing tables in BigQuery (partition + cluster)
  - Serving visuals via Looker Studio

---

**Solution (High-level)**

- Kestra flows download raw CSV â†’ clean with Pandas â†’ upload cleaned CSV to GCS â†’ load to BigQuery staging â†’ create optimized analytics table â†’ refresh dashboard.
- The flows include idempotent setup tasks (create GCS bucket, create BQ dataset) so infrastructure can be provisioned from the workflow.

---

**Architecture**

- Source: `dataset.csv` (raw CSV)
- Orchestration: Kestra workflows (YAML flows provided)
- Data Lake: Google Cloud Storage (GCS) for cleaned CSVs
- Data Warehouse: BigQuery (staging + optimized analytics table)
- Transformations: Pandas (cleaning) + BigQuery SQL (transform + optimization)
- Visualization: Looker Studio dashboard consuming the optimized BigQuery table

Mermaid flow diagram:

```mermaid
flowchart LR
  A[Raw CSV (dataset.csv)] --> B[Kestra: download_csv]
  B --> C[Kestra: clean_csv (Pandas)]
  C --> D[GCS: cleaned_dataset.csv]
  D --> E[BigQuery: staging table spotify_tracks]
  E --> F[BigQuery: spotify_tracks_clean (PARTITIONED & CLUSTERED)]
  F --> G[Looker Studio Dashboard]
```

---

**Technologies Used**

- Orchestration: Kestra
- Cloud Storage: Google Cloud Storage (GCS)
- Data Warehouse: BigQuery
- Language & Tools: Python (Pandas), SQL, Docker Compose (local dev)
- Plugins: Kestra GCP plugins (gcs, bigquery)
- Visualization: Google Looker Studio

---

## Pipeline Flow & Components

- Ingestion
  - Kestra DAG `gcp_spotify_data_ingest` handles batch ingestion.
  - `download_csv` downloads the raw CSV from GitHub.

- Cleaning
  - `clean_csv` runs a Pandas script (Kestra Python script task) that filters to precisely 20 columns and writes `cleaned_dataset.csv` with `index=False`.

- Storage
  - Cleaned CSV uploaded to GCS via `io.kestra.plugin.gcp.gcs.Upload`.
  - Staging data loaded to BigQuery using a `LOAD DATA` SQL step.

- Transformation & Optimization
  - `transform_spotify_table` creates `spotify_tracks_clean` in BigQuery using:
    - PARTITION BY RANGE_BUCKET(popularity, GENERATE_ARRAY(0, 100, 10))
    - CLUSTER BY track_genre
  - This table is the analytical table used by dashboards.

---

## Data Warehouse Optimization

- Partitioning
  - Partitioned by popularity ranges (0-9,10-19,...,90-99) via `RANGE_BUCKET` to minimize scanned data for popularity-centric queries.

- Clustering
  - Clustered on `track_genre` to speed up genre-filtered aggregations and reduce I/O.

- Benefits
  - Lower query cost and faster dashboard load times due to partition pruning and clustering locality.

---

## Dashboard (Looker Studio)

- The dashboard surfaces business-friendly analytics from the optimized BigQuery table. Minimum visualizations:
  1. Top genres by average popularity (bar chart)
  2. Tracks distribution by popularity bucket (histogram)
- Additional tiles recommended: Top artists by average valence; audio-feature trends by genre.
- Dashboard screenshot: **[Insert dashboard screenshot here]**

---

## Reproducibility â€” Setup & Run

Prerequisites:
- GCP project with BigQuery and GCS enabled
- Service account JSON with BigQuery & GCS permissions
- Docker & Docker Compose (for local Kestra + Postgres)

1) Local quick demo (Docker Compose)
```bash
cd pipeline
docker-compose up -d
docker-compose ps
```

2) Configure Kestra & GCP credentials
- Place your GCP service account JSON in a secure location and point Kestra to it.
- Replace placeholders in the Kestra KV flow (`gcp_kv`):
  - `GCP_PROJECT_ID` â€” your project id
  - `GCP_BUCKET_NAME` â€” globally unique bucket name
  - `GCP_DATASET` â€” BigQuery dataset name (e.g. spotify_data_pipeline)

3) Run Kestra flows (UI or API)
- Open Kestra UI (default: http://localhost:8080 when running via Docker Compose)
- Upload or create flows: `gcp_kv`, `gcp_setup`, `gcp_spotify_data_ingest` and execute them in order.

4) Verify BigQuery
- Confirm tables:
  - Staging: `{{GCP_PROJECT_ID}}.{{GCP_DATASET}}.spotify_tracks`
  - Analytics: `{{GCP_PROJECT_ID}}.{{GCP_DATASET}}.spotify_tracks_clean`

5) Connect Looker Studio
- Create a BigQuery data source pointing at `spotify_tracks_clean` and build the dashboard tiles.

Optional: run local Postgres ingestion (for local dev)
```bash
python -m pip install -r pipeline/spotify/requirements.txt
python pipeline/spotify/spotify_ingest.py \
  --pg_user=spotify_user \
  --pg_password=spotify_password \
  --pg_host=localhost \
  --pg_database=spotify_analytics \
  --csv_path=dataset.csv
```

---

## Dataset Description

- File: `dataset.csv` (sample Spotify tracks dataset)
- Key columns (20):
  - `track_id`, `artists`, `album_name`, `track_name`, `popularity`, `duration_ms`, `explicit`, `danceability`, `energy`, `key`, `loudness`, `mode`, `speechiness`, `acousticness`, `instrumentalness`, `liveness`, `valence`, `tempo`, `time_signature`, `track_genre`

---

## Project Features

- Orchestrated batch ingestion with Kestra
- Deterministic Pandas cleaning step to remove junk columns
- GCS data lake for cleaned CSVs
- BigQuery staging and optimized analytics table (partitioned & clustered)
- Looker Studio dashboard with multiple tiles
- Local dev via Docker Compose (Kestra, Postgres, pgAdmin)

---

## Repository Structure

- `pipeline/`
  - `docker-compose.yaml` â€” local services (Kestra, Postgres, pgAdmin, ingestion container)
  - `spotify/`
    - `dockerfile` â€” ingestion container
    - `requirements.txt`
    - `spotify_ingest.py` â€” local Postgres ingestion utility
    - `commands.sh` â€” helper commands
    - `spotify-pipiline-zoomcamp-*.json` â€” example GCP service account (replace)
- `dataset.csv` â€” sample dataset
- `README.md` â€” this file

# ðŸŽ§ Spotify Data Engineering Pipeline

A production-grade, end-to-end Spotify data engineering pipeline that ingests raw track metadata, performs deterministic cleaning, stores data in a cloud data lake and warehouse, applies BigQuery transformations and optimizations, and exposes analytics via a Looker Studio dashboard.

This repository demonstrates cloud-native batch ingestion, workflow orchestration with Kestra, BigQuery optimization (partitioning & clustering), and a reproducible workflow suitable for portfolio presentation.

---

**Project Goal**

- Build a reproducible, cloud-based batch pipeline that ingests raw Spotify CSV data, stores cleaned versions in Google Cloud Storage (GCS), loads and optimizes tables in BigQuery, and presents analytics in Looker Studio.
- Demonstrate orchestration with Kestra and deliver industry-standard data warehouse optimizations for cost-efficient analytics.

---

**Problem Description**

- Analytics stacks often fail due to inconsistent ingestion (junk/unnamed columns), manual upload steps, and unoptimized warehouse schemas that produce slow and expensive queries.
- This project automates the full batch pipeline to eliminate manual steps and ensures repeatable, low-cost analytics by:
  - Using Kestra to orchestrate a multi-step DAG
  - Cleaning data deterministically with Pandas to remove junk columns
  - Keeping a versioned clean file in GCS
  - Loading and optimizing tables in BigQuery (partition + cluster)
  - Serving visuals via Looker Studio

---

**Solution (High-level)**

- Kestra flows download raw CSV â†’ clean with Pandas â†’ upload cleaned CSV to GCS â†’ load to BigQuery staging â†’ create optimized analytics table â†’ refresh dashboard.
- The flows include idempotent setup tasks (create GCS bucket, create BQ dataset) so infrastructure can be provisioned from the workflow.

---

**Architecture**

- Source: `dataset.csv` (raw CSV)
- Orchestration: Kestra workflows (YAML flows provided)
- Data Lake: Google Cloud Storage (GCS) for cleaned CSVs
- Data Warehouse: BigQuery (staging + optimized analytics table)
- Transformations: Pandas (cleaning) + BigQuery SQL (transform + optimization)
- Visualization: Looker Studio dashboard consuming the optimized BigQuery table

Mermaid flow diagram:

```mermaid
flowchart LR
  A[Raw CSV (dataset.csv)] --> B[Kestra: download_csv]
  B --> C[Kestra: clean_csv (Pandas)]
  C --> D[GCS: cleaned_dataset.csv]
  D --> E[BigQuery: staging table spotify_tracks]
  E --> F[BigQuery: spotify_tracks_clean (PARTITIONED & CLUSTERED)]
  F --> G[Looker Studio Dashboard]
```

---

**Technologies Used**

- Orchestration: Kestra
- Cloud Storage: Google Cloud Storage (GCS)
- Data Warehouse: BigQuery
- Language & Tools: Python (Pandas), SQL, Docker Compose (local dev)
- Plugins: Kestra GCP plugins (gcs, bigquery)
- Visualization: Google Looker Studio

---

## Pipeline Flow & Components

- Ingestion
  - Kestra DAG `gcp_spotify_data_ingest` handles batch ingestion.
  - `download_csv` downloads the raw CSV from GitHub.

- Cleaning
  - `clean_csv` runs a Pandas script (Kestra Python script task) that filters to precisely 20 columns and writes `cleaned_dataset.csv` with `index=False`.

- Storage
  - Cleaned CSV uploaded to GCS via `io.kestra.plugin.gcp.gcs.Upload`.
  - Staging data loaded to BigQuery using a `LOAD DATA` SQL step.

- Transformation & Optimization
  - `transform_spotify_table` creates `spotify_tracks_clean` in BigQuery using:
    - PARTITION BY RANGE_BUCKET(popularity, GENERATE_ARRAY(0, 100, 10))
    - CLUSTER BY track_genre
  - This table is the analytical table used by dashboards.

---

## Data Warehouse Optimization

- Partitioning
  - Partitioned by popularity ranges (0-9,10-19,...,90-99) via `RANGE_BUCKET` to minimize scanned data for popularity-centric queries.

- Clustering
  - Clustered on `track_genre` to speed up genre-filtered aggregations and reduce I/O.

- Benefits
  - Lower query cost and faster dashboard load times due to partition pruning and clustering locality.

---

## Dashboard (Looker Studio)

- The dashboard surfaces business-friendly analytics from the optimized BigQuery table. Minimum visualizations:
  1. Top genres by average popularity (bar chart)
  2. Tracks distribution by popularity bucket (histogram)
- Additional tiles recommended: Top artists by average valence; audio-feature trends by genre.
- Dashboard screenshot: **[Insert dashboard screenshot here]**

---

## Reproducibility â€” Setup & Run

Prerequisites:
- GCP project with BigQuery and GCS enabled
- Service account JSON with BigQuery & GCS permissions
- Docker & Docker Compose (for local Kestra + Postgres)

1) Local quick demo (Docker Compose)
```bash
cd pipeline
docker-compose up -d
docker-compose ps
```

2) Configure Kestra & GCP credentials
- Place your GCP service account JSON in a secure location and point Kestra to it.
- Replace placeholders in the Kestra KV flow (`gcp_kv`):
  - `GCP_PROJECT_ID` â€” your project id
  - `GCP_BUCKET_NAME` â€” globally unique bucket name
  - `GCP_DATASET` â€” BigQuery dataset name (e.g. spotify_data_pipeline)

3) Run Kestra flows (UI or API)
- Open Kestra UI (default: http://localhost:8080 when running via Docker Compose)
- Upload or create flows: `gcp_kv`, `gcp_setup`, `gcp_spotify_data_ingest` and execute them in order.

4) Verify BigQuery
- Confirm tables:
  - Staging: `{{GCP_PROJECT_ID}}.{{GCP_DATASET}}.spotify_tracks`
  - Analytics: `{{GCP_PROJECT_ID}}.{{GCP_DATASET}}.spotify_tracks_clean`

5) Connect Looker Studio
- Create a BigQuery data source pointing at `spotify_tracks_clean` and build the dashboard tiles.

Optional: run local Postgres ingestion (for local dev)
```bash
python -m pip install -r pipeline/spotify/requirements.txt
python pipeline/spotify/spotify_ingest.py \
  --pg_user=spotify_user \
  --pg_password=spotify_password \
  --pg_host=localhost \
  --pg_database=spotify_analytics \
  --csv_path=dataset.csv
```

---

## Dataset Description

- File: `dataset.csv` (sample Spotify tracks dataset)
- Key columns (20):
  - `track_id`, `artists`, `album_name`, `track_name`, `popularity`, `duration_ms`, `explicit`, `danceability`, `energy`, `key`, `loudness`, `mode`, `speechiness`, `acousticness`, `instrumentalness`, `liveness`, `valence`, `tempo`, `time_signature`, `track_genre`

---

## Project Features

- Orchestrated batch ingestion with Kestra
- Deterministic Pandas cleaning step to remove junk columns
- GCS data lake for cleaned CSVs
- BigQuery staging and optimized analytics table (partitioned & clustered)
- Looker Studio dashboard with multiple tiles
- Local dev via Docker Compose (Kestra, Postgres, pgAdmin)

---

## Repository Structure

- `pipeline/`
  - `docker-compose.yaml` â€” local services (Kestra, Postgres, pgAdmin, ingestion container)
  - `spotify/`
    - `dockerfile` â€” ingestion container
    - `requirements.txt`
    - `spotify_ingest.py` â€” local Postgres ingestion utility
    - `commands.sh` â€” helper commands
    - `spotify-pipiline-zoomcamp-*.json` â€” example GCP service account (replace)
- `dataset.csv` â€” sample dataset
- `README.md` â€” this file

---

## Kestra Flows Included (summary)

- `gcp_kv` â€” seeds KV store with `GCP_PROJECT_ID`, `GCP_LOCATION`, `GCP_BUCKET_NAME`, `GCP_DATASET`.
- `gcp_setup` â€” creates GCS bucket & BigQuery dataset (idempotent `ifExists: SKIP`).
- `gcp_spotify_data_ingest` â€” end-to-end DAG:
  - `download_csv` (HTTP download)
  - `clean_csv` (Pandas script)
  - `upload_to_gcs` (GCS upload)
  - `load_to_bq` (LOAD DATA INTO staging)
  - `transform_spotify_table` (CREATE OR REPLACE partitioned & clustered table)

---

## Evaluation Criteria Coverage (explicit mapping)

- Problem description: **4 / 4** â€” Clear problem statement and automated solution.
- Cloud: **4 / 4** â€” Uses GCS & BigQuery; includes Kestra flows to provision resources (IaC-style automation via Kestra).
- Data ingestion (Batch / Workflow orchestration): **4 / 4** â€” Kestra DAG provides an end-to-end DAG with multiple steps and uploads to GCS.
- Data warehouse: **4 / 4** â€” Analytics table is partitioned by popularity ranges and clustered by `track_genre`; explanation provided.
- Transformations: **2 / 4** â€” Uses Pandas for cleaning and BigQuery SQL for transformation. (To reach 4/4, add dbt or Spark models.)
- Dashboard: **4 / 4** â€” Dashboard specified with at least two tiles (Top genres by popularity; popularity distribution).
- Reproducibility: **4 / 4** â€” Step-by-step setup for local and GCP runs; Kestra flows and Docker Compose included.

Overall: This repository demonstrates a complete, cloud-based batch pipeline with orchestration, storage, optimized warehousing, and visualization â€” ready for portfolio presentation. Transformations can be upgraded to dbt to reach full marks in that dimension.

---

## Future Improvements

- Add dbt models for transformations, testing, and lineage (raises Transformations to 4/4).
- Integrate data quality checks (Great Expectations) into Kestra flows.
- Add CI (GitHub Actions) to run basic flow smoke tests and linting.
- Automate Looker Studio dashboard provisioning or export templates.

---

## Notes & Placeholders

- Replace GCP placeholders in Kestra KV flow with your values (project id, bucket name, dataset).
- Insert the Looker Studio dashboard screenshot in place of the placeholder.

---

If you'd like, I can also:
- add a `dbt` skeleton to the repo to formalize transformations,
- add a GitHub Actions CI workflow to validate the cleaning script,
- or commit this README to the repository for you (already done).

GCP_PROJECT_ID
GCP_BUCKET_NAME
GCP_DATASET
GCP_LOCATION

5. Run Pipeline

Execute: 

gcp_spotify_data_ingest

Output

Cleaned dataset stored in Google Cloud Storage

Raw table in BigQuery

Optimized analytics table in BigQuery

Looker Studio dashboard

âœ… Project Features

End-to-end batch data pipeline

Workflow orchestration

Data cleaning and validation

Cloud data warehouse storage

Query optimization (partitioning + clustering)

Analytics dashboard

Fully reproducible setup

ðŸ“ˆ Future Improvements

Infrastructure as Code (Terraform)

dbt transformation layer

Streaming ingestion pipeline

Machine learning features
